# Routage et Annonces BGP

Ce TP se déroule sur un cluster <ins>**KinD**<ins>. 

## Sommaire
  * [But du TP](#but-du-tp)
  * [Etape 0 Préparation](#etape-0-préparation)
  * [Etape 1 Lancement de KinD](#etape-1-lancement-de-kind)
  * [Etape 2 Déploiement de Containerlab](#deploiement-de-containerlab)
  * [Etape 3 Installation de Cilium](#installation-de-cilium)
  * [Etape 4 Affichage des routes BGP](#affichage-des-routes-bgp)
  * [Etape 5 Activation de BGP dans Cilium](#activation-de-bgp-dans-cilium)
  * [Etape 6 Vérification du routage](#verification-du-routage)

## But du TP
Construire en lab une mini matrice de Clos (1 spine, 2leaf), y connecter 4 Nodes.  
Activer BGP au niveau des Nodes pour diffuser les réseaux CIDRs des Pods.

## Etape 0 Préparation

Connectez-vous en SSH sur la VM auquel l'animateur vous donne accès.  
[Containerlab](containerlab.io) et [KinD](https://kind.sigs.k8s.io/) y sont déjà installés.

## Etape 1 Lancement de KinD

Nettoyons les eventuels clusters déjà présents :
```shell
cd /home/cilium_lab/bgp-cplane-demo
./clean-kind.sh 
```

Lançons le cluster avec un script simple :
```shell
./01-build-kind-cluster.sh 
```
Nous finissons pas obtenir :
```
[..]
---
Kubernetes control plane is running at https://127.0.0.1:36969
CoreDNS is running at https://127.0.0.1:36969/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
---
NAME                                 STATUS     ROLES           AGE   VERSION
clab-bgp-cplane-demo-control-plane   NotReady   control-plane   28s   v1.24.0
clab-bgp-cplane-demo-worker          NotReady   <none>          5s    v1.24.0
clab-bgp-cplane-demo-worker2         NotReady   <none>          5s    v1.24.0
clab-bgp-cplane-demo-worker3         NotReady   <none>          5s    v1.24.0
```

Les noeuds restent en `NotReady` car le CNI n’est pas encore installé

Nous avons appliqué un label nommé `rack`, qui permet de grouper les `server` :
```shell
# kubectl get node --show-labels
NAME                                 STATUS   ROLES           AGE   VERSION   LABELS
clab-bgp-cplane-demo-control-plane   Ready    control-plane   32m   v1.24.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=clab-bgp-cplane-demo-control-plane,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,rack=rack0
clab-bgp-cplane-demo-worker          Ready    <none>          32m   v1.24.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=clab-bgp-cplane-demo-worker,kubernetes.io/os=linux,rack=rack0
clab-bgp-cplane-demo-worker2         Ready    <none>          32m   v1.24.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=clab-bgp-cplane-demo-worker2,kubernetes.io/os=linux,rack=rack1
clab-bgp-cplane-demo-worker3         Ready    <none>          32m   v1.24.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=clab-bgp-cplane-demo-worker3,kubernetes.io/os=linux,rack=rack1
```

## Etape 2 Déploiement de Containerlab

Créeons un lab avec containerlab, consitué de 4 Node (`server0` à `server3`), chaque paire étant connectée sur un switch ToR (`tor0` et `tor1`), eux-même interconnectés sur un routeur central, `router0`.

La topologie est décrite dans le fichier `topo.yaml`.

```shell
./02-deploy-containerlab.sh 
```

On obtient :

![](/img/containerlab.png)

## Etape 3 Installation de Cilium

Installons le CNI Cilium :
```shell
./03-install-cilium.sh 
```

Pour cela, nous avons utilisé le fichier Helm values, constitué ainsi :
```yaml
tunnel: disabled
ipv4NativeRoutingCIDR: "10.0.0.0/8"
bgpControlPlane:
  enabled: true
k8s:
  requireIPv4PodCIR: true
ipam:
  mode: kubernetes
hubble:
  enabled: true
  relay:
    enabled: true
  ui:
    enabled: true
```

Au final, cilium est prêt :

```shell
# cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:         OK
 \__/¯¯\__/    Operator:       OK
 /¯¯\__/¯¯\    Hubble:         OK
 \__/¯¯\__/    ClusterMesh:    disabled
    \__/

Deployment        hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet         cilium             Desired: 4, Ready: 4/4, Available: 4/4
Deployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: 4
                  hubble-relay       Running: 1
                  cilium-operator    Running: 2
                  hubble-ui          Running: 1
```

## Etape 4 Affichage des routes BGP

Nous avons crée un simple script qui se connecte dans un des switch (`tor0`, `tor1` ou `router0`) et affiche la table BGP (`show bgp ipv4 summary wide` , `show bgp ipv4 wide`)
```shell
./showbgp.sh router0
```
On obtient cela :
![](/img/bgp1.png)

On constate le peering vers `tor0` et `tor1`.

Regardons vu de `tor0` :
```shell
./showbgp.sh tor0
```

On obtient cela :
![](/img/bgp2.png)

Nous constatons qu'aucun préfixe n'est échange avec les `server0` et `server1`.

## Etape 5 Activation de BGP dans Cilium

Activons BGP au niveau des Nodes grâce aux 2 politiques Cilium suivantes (1 par rack) :
```yaml
## cilium-bgp-peering-policies.yaml 
---
apiVersion: "cilium.io/v2alpha1"
kind: CiliumBGPPeeringPolicy
metadata:
  name: rack0
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      rack: "rack0"
  virtualRouters:
    - localASN: 65010
      exportPodCIDR: true
      neighbors:
        - peerAddress: "10.0.0.1/32"
          peerASN: 65010
---
apiVersion: "cilium.io/v2alpha1"
kind: CiliumBGPPeeringPolicy
metadata:
  name: "rack1"
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      rack: rack1
  virtualRouters:
    - localASN: 65011
      exportPodCIDR: true
      neighbors:
        - peerAddress: "10.0.0.2/32"
          peerASN: 65011
```

Appliquons cette politique :
```shell
# ./04-enable-bgp-cilium.sh
ciliumbgppeeringpolicy.cilium.io/rack0 created
ciliumbgppeeringpolicy.cilium.io/rack1 created 
```
## Etape 6 Vérification du routage

Regardons à nouveau les routeur sur `router0` :  

![](/img/bgp3.png)

Nous voyons les CIDRs des Pod apparaitre !

Nous laissons au lecteur le soin de déployer un DaemonSet avec un Pod (image=nginx par exemple) par Node, et de vérifier que les préfixes sont bien échangés en réalisant des connexions TCP/80 internodes.


[Revenir au sommaire](../README.md) | [TP Suivant](./TP14.md)