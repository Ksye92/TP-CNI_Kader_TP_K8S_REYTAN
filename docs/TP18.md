# Cilium avec et sans eBPF

## kind

## Sans EBPF

Vérifier qu'aucun cluster kind ne tourne :

```bash
kind get clusters
```

Si besoin effacer ceux qui seraient en cours :

```bash
kind delete cluster --name <cluster-name>
```
On utilise un config kind simple :
    
    ```yaml
    ## kind-config.yaml 
    kind: Cluster
name: basic
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
#    listenAddress: 127.0.0.1
    protocol: TCP
  - containerPort: 443
    hostPort: 443
#    listenAddress: 127.0.0.1
    protocol: TCP
- role: worker
- role: worker
- role: worker
networking:
  apiServerAddress: "0.0.0.0"
  apiServerPort: 6443
  disableDefaultCNI: true
  ```

Lancer un cluster sans CNI :

```bash
kind create cluster --config=kind-config.yaml
```
Au bout de quelques instant le cluster répond, mais n'est pas Ready :
    
    ```bash
    # kubectl get nodes
NAME                  STATUS     ROLES           AGE     VERSION
basic-control-plane   NotReady   control-plane   9m32s   v1.24.0
basic-worker          NotReady   <none>          9m8s    v1.24.0
basic-worker2         NotReady   <none>          9m9s    v1.24.0
basic-worker3         NotReady   <none>          9m9s    v1.24.0
```

Installer cilim par défaut

```bash

helm install  --namespace kube-system --repo https://helm.cilium.io cilium cilium --version 1.12.1

```
Au bout de quelques secondes le cluster est Ready :
    
    ```bash
    # kubectl get nodes
NAME                  STATUS   ROLES           AGE   VERSION
basic-control-plane   Ready    control-plane   12m   v1.24.0
basic-worker          Ready    <none>          12m   v1.24.0
basic-worker2         Ready    <none>          12m   v1.24.0
basic-worker3         Ready    <none>          12m   v1.24.0
```

Cilium est également OK :
        
        ```bash
        # cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:         OK
 \__/¯¯\__/    Operator:       OK
 /¯¯\__/¯¯\    Hubble:         disabled
 \__/¯¯\__/    ClusterMesh:    disabled
    \__/

Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet         cilium             Desired: 4, Ready: 4/4, Available: 4/4
Containers:       cilium             Running: 4
                  cilium-operator    Running: 2
Cluster Pods:     3/3 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.12.1@sha256:ea2db1ee21b88127b5c18a96ad155c25485d0815a667ef77c2b7c7f31cab601b: 4
                  cilium-operator    quay.io/cilium/operator-generic:v1.12.1@sha256:93d5aaeda37d59e6c4325ff05030d7b48fabde6576478e3fdbfb9bb4a68ec4a1: 2
      ```
## Vérication de la non activation de eBPF

Il suffit de regarder la conf :
        
        ```bash
# cilium config view | egrep -i "(proxy|ebpf|tunnel|dsr)"
bpf-lb-mode                                    dsr
enable-l7-proxy                                true
kube-proxy-replacement                         strict
kube-proxy-replacement-healthz-bind-address    
sidecar-istio-proxy-image                      cilium/istio_proxy
tofqdns-proxy-response-max-delay               100ms
tunnel                                         disabled
```

## Avec EBPF

On efface le cluster précedent

```bash
kind delete cluster --name basic
```

On lance un nouveau cluster toujours sans CNI

```bash
kind create cluster --config=kind-config.yaml
```

Le cluster est accessible: 

```bash
# kubectl cluster-info
Kubernetes control plane is running at https://0.0.0.0:6443
CoreDNS is running at https://0.0.0.0:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

Cette fois-ci on installe cilium avec eBPF

```bash

helm upgrade --install --namespace kube-system --repo https://helm.cilium.io cilium cilium --values ebpf-values.yaml

```

Au bout d'un moment, le cluster converge :
    
    ```bash

cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:         OK
 \__/¯¯\__/    Operator:       OK
 /¯¯\__/¯¯\    Hubble:         OK
 \__/¯¯\__/    ClusterMesh:    disabled
    \__/

Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet         cilium             Desired: 4, Ready: 4/4, Available: 4/4
Deployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1
Containers:       cilium             Running: 4
                  cilium-operator    Running: 2
                  hubble-relay       Running: 1
Cluster Pods:     4/4 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.12.1@sha256:ea2db1ee21b88127b5c18a96ad155c25485d0815a667ef77c2b7c7f31cab601b: 4
                  cilium-operator    quay.io/cilium/operator-generic:v1.12.1@sha256:93d5aaeda37d59e6c4325ff05030d7b48fabde6576478e3fdbfb9bb4a68ec4a1: 2
                  hubble-relay       quay.io/cilium/hubble-relay:v1.12.1@sha256:646582b22bf41ad29dd7739b12aae77455ee5757b9ee087f2d45d684afef5fa1: 1
```



 ## DSR

vxlan doit être désactivé
https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/

# cilium config view | egrep -i "(proxy|ebpf|tunnel|dsr)"
bpf-lb-mode                                    dsr
enable-l7-proxy                                true
kube-proxy-replacement                         strict
kube-proxy-replacement-healthz-bind-address    
sidecar-istio-proxy-image                      cilium/istio_proxy
tofqdns-proxy-response-max-delay               100ms
tunnel                                         disabled
