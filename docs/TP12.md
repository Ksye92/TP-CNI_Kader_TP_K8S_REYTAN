# eBPF

## Pre-requis
Une app doit être deployée

Verifier que sur les noeuds, le cgroup pour eBPF est bien monté :
```
root@default-pool-7x6s8:~# ls /run/cilium/ -lh
total 8.0K
srw-rw----  1 root 1337    0 Sep  6 06:29 access_log.sock
-rw-r--r--  1 root root 1.3K Sep  6 05:40 bootstrap.pb
* drwxr-xr-x  2 root root   40 Sep  5 09:54 cgroupv2*
-rw-r-----  1 root root    2 Sep  6 06:29 cilium.pid
srw-rw----  1 root root    0 Sep  6 06:29 cilium.sock
srwxr-xr-x  1 root root    0 Sep  6 05:40 envoy-admin.sock
srw-rw----  1 root root    0 Sep  6 06:29 health.sock
srw-rw----  1 root root    0 Sep  6 06:29 hubble.sock
srw-rw----  1 root root    0 Sep  6 06:29 monitor1_2.sock
drwxr-x--- 15 root root  480 Sep  6 06:29 state
srw-rw----  1 root 1337    0 Sep  6 06:29 xds.sock
```

## Voir la config actuelle
    
    ```bash
    cilium config view
    ```

## Outils de debug

## Remplacement du kube-proxy
Se connecter sur un noeud et constater que les svc sont en fait des règles `iptables`

    ```bash
    sudo iptables-save | grep KUBE-SVC
    ```
C'est `kube-proxy` qui les met en place.

Voyons de plus près la config cilium

    ```bash
    kubectl exec -it -n kube-system cilium-xxxx -- cilium status verbose
    ```
Voici ce qu'on constate
```
[..]
KubeProxyReplacement Details:
  Status:             Partial
  Session Affinity:   Enabled
  Services:
  - ClusterIP:      Enabled
  - NodePort:       Disabled 
  - LoadBalancer:   Disabled 
  - externalIPs:    Disabled 
  - HostPort:       Disabled
[..]
  ```
Nous pouvons vérifier la mise en place d'eBPF :
```bash
 kubectl exec -it -n kube-system cilium-zfbcz -- cilium service list 
 ```

 qui donne quelque chose comme cela :
 ```
ID   Frontend              Service Type   Backend                  
1    10.245.0.1:443        ClusterIP      1 => 100.65.26.65:443    
2    10.245.0.10:53        ClusterIP      1 => 10.244.0.40:53      
                                          2 => 10.244.0.193:53     
3    10.245.0.10:9153      ClusterIP      1 => 10.244.0.40:9153    
                                          2 => 10.244.0.193:9153   
4    10.245.104.4:80       ClusterIP      1 => 10.244.0.114:4245   
5    10.245.251.221:80     ClusterIP      1 => 10.244.0.143:8081   
6    10.245.169.198:6379   ClusterIP      1 => 10.244.0.7:6379     
7    10.245.8.52:6379      ClusterIP      1 => 10.244.0.5:6379     
                                          2 => 10.244.0.191:6379   
8    10.245.242.46:80      ClusterIP      1 => 10.244.0.49:80      
                                          2 => 10.244.0.179:80     
                                          3 => 10.244.0.9:80       
9    10.245.4.31:8080      ClusterIP      1 => 10.244.0.223:8080   
10   10.245.15.189:8080    ClusterIP      1 => 10.244.0.94:8080    
```

`cilium-monitor` est un outil de debug qui permet de voir les évenements envoyés par le kernel vers cilium

    ```bash
    kubectl exec -it -n kube-system cilium-xxxx -- cilium monitor -v
    ```

## Masquerading IPtables => eBPF

### Etat initial
On constate que c'est IP-tables qui gère le masquerading en nous connectant sur un des Nodes (grâce à `nsenter`)

```bash
     root@default-pool-7x6su:/home/cilium# iptables-save | grep MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A CILIUM_POST_nat -s 10.244.0.128/25 ! -d 10.244.0.0/16 ! -o cilium_+ -m comment --comment "cilium masquerade non-cluster" -j MASQUERADE
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully 
```

Cela est d'ailleurs confirmé par la configuration de Cilium :
    ```bash
    kubectl exec -it -n kube-system cilium-xxxx -- cilium status | grep Masquerading
    Masquerading:           IPTables [IPv4: Enabled, IPv6: Disabled]
```

TODO : faire la manip de vérification avant

### Activation d'eBPF pour le MASQUERADE

Pour changer cette configuration, *depuis l'extérieur du cluster*, il faut modifier la configmap de cilium

    ```bash
cilium config set enable-bpf-masquerade true
cilium config set install-iptables-rules false
```

Cela donne :
```
✨ Patching ConfigMap cilium-config with enable-bpf-masquerade=true...
♻️  Restarted Cilium pods
   ```

   Les pods cilium sont redemarrés
   ```bash
   cilium config view | grep -i masq
enable-bpf-masquerade                  true
masquerade                             true
   ```
### Verification

Pour cela il faut :
1. creer un Pod nommé TEST sur lequel on va se connecter et faire un requete HTTP vers 1.1.1.1 (par ex)
2. se connecter sur le pod cilium sur le même Node que le Pod TEST et voir les paquets passer :

```bash
 % kubectl exec -it cilium-kq82q -n kube-system -- cilium monitor | grep 1.1.1.1
-> stack flow 0xd2227d53 identity 20827->world state new ifindex 0 orig-ip 0.0.0.0: 10.244.0.97:41804 -> 1.1.1.1:80 tcp SYN
-> endpoint 1777 flow 0x0 identity world->20827 state reply ifindex lxc48c25b0c0097 orig-ip 1.1.1.1: 1.1.1.1:80 -> 10.244.0.97:41804 tcp SYN, ACK
-> stack flow 0xd2227d53 identity 20827->world state established ifindex 0 orig-ip 0.0.0.0: 10.244.0.97:41804 -> 1.1.1.1:80 tcp ACK
-> stack flow 0xd2227d53 identity 20827->world state established ifindex 0 orig-ip 0.0.0.0: 10.244.0.97:41804 -> 1.1.1.1:80 tcp ACK
-> endpoint 1777 flow 0x0 identity world->20827 state reply ifindex lxc48c25b0c0097 orig-ip 1.1.1.1: 1.1.1.1:80 -> 10.244.0.97:41804 tcp ACK
-> stack flow 0xd2227d53 identity 20827->world state established ifindex 0 orig-ip 0.0.0.0: 10.244.0.97:41804 -> 1.1.1.1:80 tcp ACK, FIN
-> endpoint 1777 flow 0x0 identity world->20827 state reply ifindex lxc48c25b0c0097 orig-ip 1.1.1.1: 1.1.1.1:80 -> 10.244.0.97:41804 tcp ACK, FIN
-> stack flow 0xd2227d53 identity 20827->world state established ifindex 0 orig-ip 0.0.0.0: 10.244.0.97:41804 -> 1.1.1.1:80 tcp ACK
```

On peut aussi lister les entrées Conntrack de eBPF :

```bash
kubectl exec -it cilium-kq82q -n kube-system -- cilium bpf ct list global | grep 1.1.1.1
```





## Activation de DSR

helm install cilium cilium/cilium --version 1.10.14 \
    --namespace kube-system \
    --set tunnel=disabled \
    --set autoDirectNodeRoutes=true \
    --set kubeProxyReplacement=strict \
    --set loadBalancer.mode=dsr \
    --set k8sServiceHost=REPLACE_WITH_API_SERVER_IP \
    --set k8sServicePort=REPLACE_WITH_API_SERVER_PORT
