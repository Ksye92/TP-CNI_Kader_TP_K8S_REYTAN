# eBPF
Ce TP se déroule sur KinD.
//TO-DO


## Création du Cluster

Nettoyons les eventuels clusters déjà présents :
```shell
cd /mnt/cilium_lab/basic
./clean-kind.sh 
```

Lançons un cluster *sans* CNI :
```shell
./01-install-cluster.sh
```

Analyser les valeurs présentes dans ebpf-values.yaml et installer le CNI :
```shell
helm upgrade --install --namespace kube-system --repo https://helm.cilium.io cilium cilium --version 1.12.1 --values ebpf-values.yaml"
```

## Déploiement de l'app

Deployons le frontend de Guestbook, replica à 1 et svc en NodePort :
```bash
kubectl apply -f frontend-deployment.yaml 
kubectl apply -f frontend-svc.yaml
```

Voyons le svc :
```bash
kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
frontend     NodePort    10.96.188.84   <none>        80:32150/TCP   107s
```
L'appli est publiée en NodePort sur le port TCP/32150.

Observons les logs (cf (TP11)[./TP11.md]).

Depuis le shell de la VM , requêtons le NodePort sur tous les noeuds

On constate que dans tous les cas, c'est bien l'IP du visiteur (== la VM) qui est vu dans le Pod :
```
172.18.0.1 - - [14/Sep/2022:20:36:33 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.81.0"
172.18.0.1 - - [14/Sep/2022:20:36:51 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.81.0"
172.18.0.1 - - [14/Sep/2022:20:36:54 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.81.0"
172.18.0.1 - - [14/Sep/2022:20:36:56 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.81.0"
```

Cette preservation est possible grâce à eBPF.

Si vous sniffez le veth (veth71bd8ea) du Node (172.18.0.2) qui porte le Pod frontend (10.244.4.66) alors que la VM (172.18.0.1) accède au Nodeport (32150)sur un autre Node (172.18.0.3) vous constaterez le DSR :
```
# tcpdump -n -i veth71bd8ea tcp port 80 or tcp port 32150
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on veth71bd8ea, link-type EN10MB (Ethernet), snapshot length 262144 bytes
22:53:09.540087 IP 172.18.0.1.52216 > 10.244.4.66.80: Flags [S], seq 645493402, win 64240, options [mss 1460,sackOK,TS val 2501130328 ecr 0,nop,wscale 7], length 0
22:53:09.540269 IP 172.18.0.3.32150 > 172.18.0.1.52216: Flags [S.], seq 1122188459, ack 645493403, win 65160, options [mss 1460,sackOK,TS val 1158515731 ecr 2501130328,nop,wscale 7], length 0
22:53:09.540326 IP 172.18.0.1.52216 > 10.244.4.66.80: Flags [.], ack 1122188460, win 502, options [nop,nop,TS val 2501130328 ecr 1158515731], length 0
```
Autre essai :
```shell
# kubectl exec -ti pod/cilium-d54wb -n kube-system -- cilium monitor
Defaulted container "cilium-agent" out of: cilium-agent, mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init)
Listening for events on 4 CPUs with 64x4096 of shared memory
Press Ctrl-C to quit
level=info msg="Initializing dissection cache..." subsys=monitor
-> stack flow 0xa287ddaf , identity health->host state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.89:4240 -> 10.244.4.222:41480 tcp ACK
-> endpoint 344 flow 0xf9cdfe0b , identity host->health state established ifindex lxc_health orig-ip 10.244.4.222: 10.244.4.222:41480 -> 10.244.4.89:4240 tcp ACK
-> stack flow 0x6df261a9 , identity health->remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.89:4240 -> 172.18.0.5:38784 tcp ACK
-> endpoint 344 flow 0x822769ff , identity remote-node->health state established ifindex lxc_health orig-ip 172.18.0.5: 172.18.0.5:38784 -> 10.244.4.89:4240 tcp ACK
-> stack flow 0x279b4232 , identity health->remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.89:4240 -> 172.18.0.4:37782 tcp ACK
-> endpoint 344 flow 0xae640a75 , identity remote-node->health state established ifindex lxc_health orig-ip 172.18.0.4: 172.18.0.4:37782 -> 10.244.4.89:4240 tcp ACK
-> endpoint 1890 flow 0xe0d23007 , identity world->6623 state new ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:59700 -> 10.244.4.66:80 tcp SYN
-> stack flow 0x3f7084a , identity 6623->world state reply ifindex 0 orig-ip 0.0.0.0: 172.18.0.3:32150 -> 172.18.0.1:59700 tcp SYN, ACK
-> endpoint 1890 flow 0xe0d23007 , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:59700 -> 10.244.4.66:80 tcp ACK
-> endpoint 1890 flow 0xe0d23007 , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:59700 -> 10.244.4.66:80 tcp ACK
-> stack flow 0x3f7084a , identity 6623->world state reply ifindex 0 orig-ip 0.0.0.0: 172.18.0.3:32150 -> 172.18.0.1:59700 tcp ACK
-> endpoint 1890 flow 0xe0d23007 , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:59700 -> 10.244.4.66:80 tcp ACK, FIN
-> stack flow 0x3f7084a , identity 6623->world state reply ifindex 0 orig-ip 0.0.0.0: 172.18.0.3:32150 -> 172.18.0.1:59700 tcp ACK, FIN
-> endpoint 1890 flow 0xe0d23007 , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:59700 -> 10.244.4.66:80 tcp ACK
-> stack flow 0x7523aecb , identity health->kube-apiserver state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.89:4240 -> 172.18.0.3:34748 tcp ACK
-> endpoint 344 flow 0xde93bc3f , identity kube-apiserver->health state established ifindex lxc_health orig-ip 172.18.0.3: 172.18.0.3:34748 -> 10.244.4.89:4240 tcp ACK
>> IPCache entry upserted: {"cidr":"172.18.0.2/32","id":1,"old-id":1,"encrypt-key":0}
>> IPCache entry upserted: {"cidr":"10.244.4.222/32","id":1,"old-id":1,"encrypt-key":0}
>> IPCache entry upserted: {"cidr":"0.0.0.0/0","id":2,"old-id":2,"encrypt-key":0}
-> endpoint 344 flow 0xf9cdfe0b , identity host->health state established ifindex lxc_health orig-ip 10.244.4.222: 10.244.4.222:41480 -> 10.244.4.89:4240 tcp ACK
-> stack flow 0xa287ddaf , identity health->host state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.89:4240 -> 10.244.4.222:41480 tcp ACK
-> stack flow 0x6df261a9 , identity health->remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.89:4240 -> 172.18.0.5:38784 tcp ACK
-> endpoint 344 flow 0x822769ff , identity remote-node->health state established ifindex lxc_health orig-ip 172.18.0.5: 172.18.0.5:38784 -> 10.244.4.89:4240 tcp ACK
```

# cilium monitor -v --related-to 1890
Listening for events on 4 CPUs with 64x4096 of shared memory
Press Ctrl-C to quit
level=info msg="Initializing dissection cache..." subsys=monitor
-> endpoint 1890 flow 0x225d6fff , identity world->6623 state new ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:38138 -> 10.244.4.66:80 tcp SYN
-> network flow 0x503498ae , identity 6623->world state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.66:80 -> 172.18.0.1:38138 tcp SYN, ACK
-> endpoint 1890 flow 0x225d6fff , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:38138 -> 10.244.4.66:80 tcp ACK
-> endpoint 1890 flow 0x225d6fff , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:38138 -> 10.244.4.66:80 tcp ACK
-> endpoint 1890 flow 0x225d6fff , identity world->6623 state established ifindex lxc39c176bde913 orig-ip 172.18.0.1: 172.18.0.1:38138 -> 10.244.4.66:80 tcp ACK, FIN
-> network flow 0x503498ae , identity 6623->world state reply ifindex 0 orig-ip 0.0.0.0: 10.244.4.66:80 -> 172.18.0.1:38138 tcp ACK


[Revenir au sommaire](../README.md) | [TP Suivant](./TP13.md)